{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from scipy import signal\n",
    "import scipy.sparse.linalg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import conv2d\n",
    "import functools\n",
    "from matplotlib.widgets import Slider, Button, RadioButtons\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crf.gaussian_matrix import LatticeGaussian, LatticeFilter#, LSHGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crf.utils import read_image, read_pfm, read_pgm\n",
    "from crf.features import Vgg16features\n",
    "from crf.crf import *\n",
    "from crf.depth import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = read_image('imL.png')#[::2,::2]\n",
    "img2 = read_image('imR.png')#[::2,::2]\n",
    "gt_depth = read_pgm('truedisp.row3.col3.pgm')\n",
    "#img1 = read_image('im0.png')[::3,::3]\n",
    "#img2 = read_image('im1.png')[::3,::3]\n",
    "#gt_depth = read_pfm('disp0.pfm')[::3,::3]\n",
    "device = torch.device('cpu')#torch.device('cuda')#torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0\n",
    "# Get random projection of VGG16 features\n",
    "VGG = Vgg16features()\n",
    "VGG.to(device)\n",
    "VGG.eval()\n",
    "features = VGG.get_features(img1,k=q+1)\n",
    "torch_features = torch.from_numpy(features[q]).to(device).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 384, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=5\n",
    "class ReferenceMatrix(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.projection = nn.Linear(64,d)\n",
    "        self.sigma_c = nn.Parameter(torch.tensor(.1).float())\n",
    "        self.sigma_p = nn.Parameter(torch.tensor(.1).float())\n",
    "    def forward(self,img,nn_features):\n",
    "        #projected_features = self.projection(nn_features)/10\n",
    "        scaled_rgb = torch.from_numpy(img).float()/self.sigma_c\n",
    "        h,w,_ = img.shape\n",
    "        ij = torch.from_numpy(np.mgrid[:h,:w].transpose((1,2,0))/np.sqrt(h**2+w**2)).float()\n",
    "        scaled_ij = ij/self.sigma_p\n",
    "        #print(ij.shape,scaled_rgb.shape, projected_features.shape)\n",
    "        return torch.cat([scaled_ij,scaled_rgb],dim=-1).reshape(h*w,5)#,projected_features\n",
    "    \n",
    "class denseCRF(nn.Module):\n",
    "    def __init__(self,n_iters=5,num_classes=48):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cpu')\n",
    "        self.n_iters = n_iters\n",
    "        self.reference = ReferenceMatrix()\n",
    "        self.labels = torch.arange(num_classes).float().to(self.device)\n",
    "        self.Mu = compatibility_matrix(partial(charbonneir,gamma=3),self.labels)\n",
    "        self.w1 = nn.Parameter(torch.tensor(1).float())\n",
    "        self.E_weight = nn.Parameter(torch.tensor(1).float())\n",
    "        \n",
    "        \n",
    "    def forward(self,E,img,nn_features):\n",
    "        ref = self.reference(img,nn_features)\n",
    "        W = LatticeGaussian(ref)\n",
    "        Q_out = mean_field_infer(E_0*self.E_weight,W,self.Mu*self.w1,self.n_iters)\n",
    "        expected_depths = Q_out@self.labels\n",
    "        return expected_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper parameters\n",
    "ws = 9      # Disparity aggregation window size\n",
    "gamma = 3    # Charbonneir turning point\n",
    "sigma_c = .1#.15#.1#.1 # Filter stdev for color channels\n",
    "sigma_p = .1#.08#.1 # Filter stdev for position channels\n",
    "sigma_f = 3#3#10#3.46#10 # Filter stdev for feature channels\n",
    "n_iters = 10 # Number of mean field message passing iterations\n",
    "down_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unary potentials from window sweep\n",
    "disp_energy = disparity_badness(img1,img2,ws,criterion=AD)\n",
    "disps = np.argmin(disp_energy,axis=-1)\n",
    "L = disp_energy.shape[-1] # Number of possible disparities\n",
    "downsampled_out = disp_energy[::down_factor,::down_factor]\n",
    "h,w,_ = downsampled_out.shape\n",
    "n = h*w\n",
    "\n",
    "E_0 = torch.from_numpy(downsampled_out.reshape(-1,L)).float().to(device)\n",
    "P_0 = F.softmax(-E_0,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     mf = mean_field_infer(E_0,W,Mu,n_iters)\n",
    "#     expected_depths = mf@labels.to(device)#.max(dim=-1)[1]#@labels.to(device)\n",
    "#     crf_depth = expected_depths.reshape(h,w).cpu().numpy()\n",
    "labels = torch.arange(L).float()\n",
    "baseline_depth = (P_0@labels.to(device)).reshape(h,w).cpu().numpy()\n",
    "D = denseCRF(n_iters=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.56878693898519\n"
     ]
    }
   ],
   "source": [
    "gt = torch.from_numpy(gt_depth.reshape(-1)).float()\n",
    "def testrun():\n",
    "    crf_depth = D(E_0,img1,torch_features);\n",
    "    diff = (4*crf_depth - gt/4)[gt!=0];\n",
    "    loss = (diff**2).mean(); loss.backward()\n",
    "#%lprun -f LatticeFilter.backward testrun()\n",
    "#%timeit -n 1 -r 2 testrun()\n",
    "# with torch.autograd.profiler.profile() as prof:\n",
    "#     testrun()\n",
    "# print(prof)\n",
    "n=3\n",
    "t0 = time.time()\n",
    "for i in range(n):\n",
    "    testrun()\n",
    "print((time.time()-t0)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
